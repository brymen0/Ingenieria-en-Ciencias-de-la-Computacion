@inproceedings{10.1145/3689092.3689393,
author = {Chen, Wuyang and Sun, Yanjie and Xu, Kele and Dou, Yong},
title = {THE-FD: Task Hierarchical Emotion-aware for Fake Detection},
year = {2024},
isbn = {9798400712036},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689092.3689393},
doi = {10.1145/3689092.3689393},
abstract = {With the rapid development of deepfake generation technology, more Realistic fine-grained short segment modifications are being observed. However, most works focus on entire video fake classification; only a few studies pay attention to precise localization for both audio-visual modalities within a video. In this paper, we present our solution not only for whole-video fake classification but also for segment-level forgery temporal localization. We propose a Task Hierarchical Emotion-aware for Fake Detection (THE-FD) architecture that initially handles video-level data and subsequently naturally adapts to segment-level data. The benefit of this approach is that it provides a general framework that allows for the inheritance and sharing of features while utilizing a shared structure. The advances of our methods include: initially introducing the emotional feature space to represent sentiment perturbations, proposing a fake-frame heatmap module to capture multimodal hidden fake patterns, and finally employing a multiscale pyramid transformer to learn features at different levels and time scales. With the benefit of these advances, our method has shown a significant improvement in performance compared to competitive methods. Our method ranks among the top-performing methods in the 2024 1M-Deepfakes Detection Challenge.},
booktitle = {Proceedings of the 2nd International Workshop on Multimodal and Responsible Affective Computing},
pages = {10–14},
numpages = {5},
keywords = {av-deepfake1m, emotion-aware detection, fake detector},
location = {Melbourne VIC, Australia},
series = {MRAC '24}
}

@inproceedings{10.1145/3689092.3689399,
author = {Lai, Zhengqin and Hong, Xiaopeng and Wang, Yabin},
title = {Multimodal Blockwise Transformer for Robust Sentiment Recognition},
year = {2024},
isbn = {9798400712036},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3689092.3689399},
doi = {10.1145/3689092.3689399},
abstract = {The MER-NOISE challenges participants to classify emotions from multimodal data, specifically audio and visual, with added noise. In this paper, we present a solution for the NOISE track of the MER2024 competition, which focuses on the robustness of emotion recognition in noisy environments. We propose a novel multimodal Blockwise Transformer (MBT) architecture, which effectively integrates visual, auditory, and textual features to improve emotion classification accuracy. Our approach includes several key innovations: the MBT network structure, the TIE module for weighted encoder input, and the momentum contrast. Additionally, we employed diverse data augmentation methods, both conventional and novel, and introduced a confidence-based decision-level fusion strategy to enhance model performance. In the MER2024 NOISE track, our solution achieved a Weighted Average F-score (WAF) of 0.8365, securing third place. This result demonstrates the effectiveness and robustness of our approach in handling noisy data for emotion recognition tasks.},
booktitle = {Proceedings of the 2nd International Workshop on Multimodal and Responsible Affective Computing},
pages = {88–92},
numpages = {5},
keywords = {modality robustness, multimodal fusion, multimodal sentiment analysis},
location = {Melbourne VIC, Australia},
series = {MRAC '24}
}

@inproceedings{10.1145/3607865.3613181,
author = {Cai, Yunrui and Xie, Jingran and Tang, Boshi and Wang, Yuanyuan and Chen, Jun and Xue, Haiwei and Wu, Zhiyong},
title = {First-order Multi-label Learning with Cross-modal Interactions for Multimodal Emotion Recognition},
year = {2023},
isbn = {9798400702884},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3607865.3613181},
doi = {10.1145/3607865.3613181},
abstract = {Multimodal emotion recognition (MER) is essential for the machine to fully understand human intentions. Various deep neural network based models are proposed but it is still challenging to better model and fuse multimodal features. In addition, recent studies have focused on the classification task of predicting discrete labels, while lacking consideration of the dimension value. In this paper, we propose a multimodal fusion model based on Transformer architecture and cross-modal interactions, and adopt a multi-label learning algorithm of first-order strategy to predict discrete labels and dimension values respectively. We also propose a semi-supervised learning method of moment injection with unlabeled data to enhance the robustness of the model. Finally, we use ensemble learning to further improve the performance of the model. We evaluate the proposed method on the MER-MULTI sub-challenge of Multimodal Emotion Recognition Challenge (MER 2023). Experimental results demonstrate the promising performance of our proposed method, which can achieve the evaluation metric of 0.6765 on the test set.},
booktitle = {Proceedings of the 1st International Workshop on Multimodal and Responsible Affective Computing},
pages = {13–20},
numpages = {8},
keywords = {multi-label learning, multimodal emotion recognition, neural networks, semi-supervised learning, transformer},
location = {Ottawa ON, Canada},
series = {MRAC '23}
}

@inproceedings{10.1145/3489517.3530490,
author = {Wei, Yijie and Zhong, Zhiwei and Gu, Jie},
title = {Human emotion based real-time memory and computation management on resource-limited edge devices},
year = {2022},
isbn = {9781450391429},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3489517.3530490},
doi = {10.1145/3489517.3530490},
abstract = {Emotional AI or Affective Computing has been projected to grow rapidly in the upcoming years. Despite many existing developments in the application space, there has been a lack of hardware-level exploitation of the user's emotions. In this paper, we propose a deep collaboration between user's affects and the hardware system management on resource-limited edge devices. Based on classification results from efficient affect classifiers on smartphone devices, novel real-time management schemes for memory, and video processing are proposed to improve the energy efficiency of mobile devices. Case studies on H.264 / AVC video playback and Android smartphone usages are provided showing significant power saving of up to 23\% and reduction of memory loading of up to 17\% using the proposed affect adaptive architecture and system management schemes.},
booktitle = {Proceedings of the 59th ACM/IEEE Design Automation Conference},
pages = {487–492},
numpages = {6},
keywords = {LSTM, affective computing, edge devices, memory management, system management, wearable devices},
location = {San Francisco, California},
series = {DAC '22}
}

@inproceedings{10.1145/3678957.3688618,
author = {Anubhav},
title = {Investigating Multi-Reservoir Computing for EEG-based Emotion Recognition},
year = {2024},
isbn = {9798400704628},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3678957.3688618},
doi = {10.1145/3678957.3688618},
abstract = {Innovations in Brain-Computer Interface (BCI) technology, including cost-effective and mobile EEG headsets, offer significant potential for monitoring mental health. My research delves into the investigation of a multi-reservoir computing network for EEG-based emotion recognition utilising several publicly available benchmark datasets. Motivation for using reservoir computing, a variant of recurrent neural networks designed to mimic human brain functioning, in a multi-reservoir network architecture lies in the notion that each reservoir represents the activity of a distinct brain area. Therefore, the proposed framework processes signals from different EEG channels yielding high-dimensional representations which are used for training traditional classifiers predicting emotions in the Valence and Arousal domains. My objectives are two fold: inspect and establish the localisation of negative/positive emotions and investigate the underlying neural mechanisms of transitions between affective states. Furthermore, my research outcomes will explore the importance of tuning generalisation vs personalisation when developing an application and will facilitate the development of real-time prediction models that require monitoring of fewer EEG channels.},
booktitle = {Proceedings of the 26th International Conference on Multimodal Interaction},
pages = {637–641},
numpages = {5},
keywords = {Affective Computing, EEG, Emotion Recognition, Genetic Algorithm, Reservoir Computing},
location = {San Jose, Costa Rica},
series = {ICMI '24}
}

@inproceedings{10.1145/3641181.3641183,
author = {Huang, Ying},
title = {Real-time application and effect evaluation of multimodal emotion recognition model in online learning},
year = {2024},
isbn = {9798400709319},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3641181.3641183},
doi = {10.1145/3641181.3641183},
abstract = {Online learning is currently developing rapidly, but the separation between teachers and students' physical spaces leads to a lack of emotional communication, which is a huge challenge that online learning currently faces. This study aims to design a multimodal emotion recognition system for online learning, to achieve real-time monitoring and feedback on the emotional states of online learners, in order to facilitate teachers' real-time adjustment and improvement of online teaching resources. This article first reviews and evaluates relevant research in the field of online learning, and then proposes a design scheme based on a multimodal emotion model, which is implemented and validated. Finally, this article verifies the effectiveness and superiority of the system through experiments.},
booktitle = {Proceedings of the 2024 10th International Conference on Computing and Data Engineering},
pages = {58–63},
numpages = {6},
keywords = {Multimodal emotion recognition, Machine learning, Real-time monitoring, Feedback, Online learning},
location = {Bangkok, Thailand},
series = {ICCDE '24}
}

@inproceedings{10.1145/3486011.3486472,
author = {Marcos, Samuel and Garc\'{\i}a Pe\~{n}alvo, Francisco Jos\'{e} and V\'{a}zquez Ingelmo, Andrea},
title = {Emotional AI in Healthcare: a pilot architecture proposal to merge emotion recognition tools},
year = {2021},
isbn = {9781450390668},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3486011.3486472},
doi = {10.1145/3486011.3486472},
abstract = {The use of emotional artificial intelligence (EAI) looks promising and is continuing to improve during the last years. However, in order to effectively use EAI to help in the diagnose and treat health conditions there are still significant challenges to be tackled. Because EAI is still under development, one of the most important challenges is to integrate the technology into the health provision process. In this sense, it is important to complement EAI technologies with expert supervision, and to provide health professionals with the necessary tools to make the best of EAI without a deep knowledge of the technology. The present work aims to provide an initial architecture proposal for making use of different available technologies for emotion recognition, where their combination could enhance emotion detection. The proposed architecture is based on an evolutionary approach so to be integrated in digital health ecosystems, so new modules can be easily integrated. In addition, internal data exchange utilizes Robot Operating System (ROS) syntax, so it can also be suitable for physical agents.},
booktitle = {Ninth International Conference on Technological Ecosystems for Enhancing Multiculturality (TEEM'21)},
pages = {342–349},
numpages = {8},
keywords = {Digital Ecosystems, Emotional AI, Healthcare, Software Architecture},
location = {Barcelona, Spain},
series = {TEEM'21}
}

@inproceedings{10.1145/3678957.3685730,
author = {Anubhav and Fujiwara, Kantaro},
title = {Across Trials vs Subjects vs Contexts: A Multi-Reservoir Computing Approach for EEG Variations in Emotion Recognition},
year = {2024},
isbn = {9798400704628},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3678957.3685730},
doi = {10.1145/3678957.3685730},
abstract = {Advancements in Brain-Computer Interface (BCI) technology, such as portable and inexpensive electroencephalogram (EEG) headsets, have profound implications for using this technology in monitoring mental health. This study delves into the development of a multi-reservoir computing network for EEG-based emotion recognition, addressing variations across trials, subjects, and contexts. We utilise several widely recognised benchmark datasets in the field, including DEAP[18], DREAMER[17], and GAMEEMO[2], selected for their diverse emotion stimuli (active/passive), EEG acquisition methods (wired/wireless), and trial sizes per subject (few/many). Our approach involves utilising reservoir computing, a variant of recurrent neural networks designed to mimic human brain functioning, in a multi-reservoir network architecture where each reservoir represents the activity of a distinct brain area. We propose a framework where signals from different EEG channels are stacked and fed into different reservoirs, yielding high-dimensional representations. These representations are used to train a support vector machine (SVM) classifier for predicting emotions in the Valence and Arousal domains. To optimise the stacking of EEG channels, we employ a Genetic Algorithm search strategy and conduct a comparative analysis to identify key EEG channels essential for efficient and computationally inexpensive predictions. We observe average classification performance across trials: 0.87 (Valence) and 0.86 (Arousal), across subjects: 0.84 (Valence) and 0.83 (Arousal) and across contexts: 0.74 (Valence) and 0.7 (Arousal). Subtle differences in classification performance indicate the importance of tuning generalisation vs personalisation when developing an application. Building upon these findings, future extensions of this work aim to facilitate the development of real-time prediction models that require monitoring of fewer EEG channels, thereby advancing the field of emotion recognition.},
booktitle = {Proceedings of the 26th International Conference on Multimodal Interaction},
pages = {518–525},
numpages = {8},
keywords = {Affective Computing, EEG, Emotion Recognition, Genetic Algorithm, Reservoir Computing},
location = {San Jose, Costa Rica},
series = {ICMI '24}
}

@inproceedings{10.1145/3637732.3637734,
author = {Dai, Weina and Wang, Tao and Yan, Feifan and Liu, Xiaoya and Liu, Shuang},
title = {Fusion Network Modeling for Cross-Time Emotion Recognition from EEG},
year = {2024},
isbn = {9798400708343},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3637732.3637734},
doi = {10.1145/3637732.3637734},
abstract = {In recent years, emotion recognition technology has played an important role in various industries. Most of the publicly available cross-temporal EEG model focus on extracting temporal or spatial features for emotion classification, while neglecting the integration of the brain’s overall structure with multidimensional information. Thus, methods based on manual feature extraction perform poorly in cross-temporal EEG emotion recognition. To address these issues, this paper proposes a novel cross-temporal EEG emotion recognition model. The model consists of a graph convolutional network and a convolutional neural network with an attention mechanism. The graph convolutional network naturally fits the brain’s structure and provides both temporal and spatial information based on channel positions. The convolutional neural network with attention mechanism explores important and discriminative features on top of enhancing the representational capacity of deep networks. Five experiments were conducted on 24 subjects, with time intervals of 1 day, 3 days, 7 days, and 14 days, resulting in a dataset of 720 cases of EEG data encompassing three emotions. The data from the initial three experiments constituted the training set, while the data from the final two experiments served as the test set. By integrating multiple networks into a unified architecture, we demonstrate enhanced classification performance. Our method achieved an average classification accuracy of 62.79\%.in the datasets used in this study. The experimental results indicate that the fused model can comprehensively and flexibly learn and process EEG signal data, providing deep and shallow features required for classification.},
booktitle = {Proceedings of the 2023 10th International Conference on Biomedical and Bioinformatics Engineering},
pages = {162–167},
numpages = {6},
keywords = {EEG, affective computing, deep learning, emotion recognition},
location = {Kyoto, Japan},
series = {ICBBE '23}
}

@inproceedings{10.1145/3503161.3548116,
author = {Chen, Yingjie and Chen, Chong and Luo, Xiao and Huang, Jianqiang and Hua, Xian-Sheng and Wang, Tao and Liang, Yun},
title = {Pursuing Knowledge Consistency: Supervised Hierarchical Contrastive Learning for Facial Action Unit Recognition},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3548116},
doi = {10.1145/3503161.3548116},
abstract = {With the increasing need for emotion analysis, facial action unit (AU) recognition has attracted much more attention as a fundamental task for affective computing. Although deep learning has boosted the performance of AU recognition to a new level in recent years, it remains challenging to extract subject-consistent representations since the appearance changes caused by AUs are subtle and ambiguous among subjects. We observe that there are three kinds of inherent relations among AUs, which can be treated as strong prior knowledge, and pursuing the consistency of such knowledge is the key to learning subject-consistent representations. To this end, we propose a supervised hierarchical contrastive learning method (SupHCL) for AU recognition to pursue knowledge consistency among different facial images and different AUs, which is orthogonal to methods focusing on network architecture design. Specifically, SupHCL contains three relation consistency modules, i.e., unary, binary, and multivariate relation consistency modules, which take the corresponding kind of inherent relations as extra supervision to encourage knowledge-consistent distributions of both AU-level and image-level representations. Experiments conducted on two commonly used AU benchmark datasets, BP4D and DISFA, demonstrate the effectiveness of each relation consistency module and the superiority of SupHCL.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {111–119},
numpages = {9},
keywords = {facial action unit, supervised contrastive learning},
location = {Lisboa, Portugal},
series = {MM '22}
}

@inproceedings{10.1145/3233824.3233829,
author = {P\'{e}rez, Joaqu\'{\i}n and Cerezo, Eva and Gallardo, Jes\'{u}s and Ser\'{o}n, Francisco J.},
title = {Evaluating an ECA with a Cognitive-Affective Architecture},
year = {2018},
isbn = {9781450364911},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3233824.3233829},
doi = {10.1145/3233824.3233829},
abstract = {In this paper, we present an embodied conversational agent (ECA) that includes a cognitive-affective architecture based on the Soar cognitive architecture, integrates an emotion model based on ALMA that uses a three-layered model of emotions, mood and personality, from the point of view of the user and the agent. These features allow to modify the behavior and personality of the agent to achieve a more realistic and believable interaction with the user. This ECA works as a virtual assistant to search information from Wikipedia and show personalized results to the user. It is only a prototipe, but can be used to show some of the possibilities of the system. A first evaluation was conducted to prove these possibilities, with satisfactory results that also give guidance for some future work that can be done with this ECA.},
booktitle = {Proceedings of the XIX International Conference on Human Computer Interaction},
articleno = {22},
numpages = {8},
keywords = {affective computing, cognitive architecture, embodied conversational agent},
location = {Palma, Spain},
series = {Interacci\'{o}n '18}
}

@inproceedings{10.1145/3382507.3417960,
author = {Wang, Yanan and Wu, Jianming and Heracleous, Panikos and Wada, Shinya and Kimura, Rui and Kurihara, Satoshi},
title = {Implicit Knowledge Injectable Cross Attention Audiovisual Model for Group Emotion Recognition},
year = {2020},
isbn = {9781450375818},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3382507.3417960},
doi = {10.1145/3382507.3417960},
abstract = {Audio-video group emotion recognition is a challenging task since it is difficult to gather a broad range of potential information to obtain meaningful emotional representations. Humans can easily understand emotions because they can associate implicit contextual knowledge (contained in our memory) when processing explicit information they can see and hear directly. This paper proposes an end-to-end architecture called implicit knowledge injectable cross attention audiovisual deep neural network (K-injection audiovisual network) that imitates this intuition. The K-injection audiovisual network is used to train an audiovisual model that can not only obtain audiovisual representations of group emotions through an explicit feature-based cross attention audiovisual subnetwork (audiovisual subnetwork), but is also able to absorb implicit knowledge of emotions through two implicit knowledge-based injection subnetworks (K-injection subnetwork). In addition, it is trained with explicit features and implicit knowledge but can easily make inferences using only explicit features. We define the region of interest (ROI) visual features and Melspectrogram audio features as explicit features, which obviously are present in the raw audio-video data. On the other hand, we define the linguistic and acoustic emotional representations that do not exist in the audio-video data as implicit knowledge. The implicit knowledge distilled by adapting video situation descriptions and basic acoustic features (MFCCs, pitch and energy) to linguistic and acoustic K-injection subnetworks is defined as linguistic and acoustic knowledge, respectively. When compared to the baseline accuracy for the testing set of 47.88\%, the average of the audiovisual models trained with the (linguistic, acoustic and linguistic-acoustic) K-injection subnetworks achieved an overall accuracy of 66.40\%.},
booktitle = {Proceedings of the 2020 International Conference on Multimodal Interaction},
pages = {827–834},
numpages = {8},
keywords = {affective computing, machine learning for multimodal interaction, multimodal fusion and representation},
location = {Virtual Event, Netherlands},
series = {ICMI '20}
}

@inproceedings{10.1145/3242969.3264978,
author = {Fan, Yingruo and Lam, Jacqueline C. K. and Li, Victor O. K.},
title = {Video-based Emotion Recognition Using Deeply-Supervised Neural Networks},
year = {2018},
isbn = {9781450356923},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3242969.3264978},
doi = {10.1145/3242969.3264978},
abstract = {Emotion recognition (ER) based on natural facial images/videos has been studied for some years and considered a comparatively hot topic in the field of affective computing. However, it remains a challenge to perform ER in the wild, given the noises generated from head pose, face deformation, and illumination variation. To address this challenge, motivated by recent progress in Convolutional Neural Network (CNN), we develop a novel deeply supervised CNN (DSN) architecture, taking the multi-level and multi-scale features extracted from different convolutional layers to provide a more advanced representation of ER. By embedding a series of side-output layers, our DSN model provides class-wise supervision and integrates predictions from multiple layers. Finally, our team ranked 3rd at the EmotiW 2018 challenge with our model achieving an accuracy of 61.1\%.},
booktitle = {Proceedings of the 20th ACM International Conference on Multimodal Interaction},
pages = {584–588},
numpages = {5},
keywords = {convolutional neural network, deeply-supervised, emotion recognition, emotiw 2018 challenge, side-output layers},
location = {Boulder, CO, USA},
series = {ICMI '18}
}

@article{10.1145/3510822,
author = {Mascarenhas, Samuel and Guimar\~{a}es, Manuel and Prada, Rui and Santos, Pedro A. and Dias, Jo\~{a}o and Paiva, Ana},
title = {FAtiMA Toolkit: Toward an Accessible Tool for the Development of Socio-emotional Agents},
year = {2022},
issue_date = {March 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {12},
number = {1},
issn = {2160-6455},
url = {https://doi.org/10.1145/3510822},
doi = {10.1145/3510822},
abstract = {More than a decade has passed since the development of FearNot!, an application designed to help children deal with bullying through role-playing with virtual characters. It was also the application that led to the creation of FAtiMA, an affective agent architecture for creating autonomous characters that can evoke empathic responses. In this article, we describe the FAtiMA Toolkit, a collection of open-source tools that is designed to help researchers, game developers, and roboticists incorporate a computational model of emotion and decision-making in their work. The toolkit was developed with the goal of making FAtiMA more accessible, easier to incorporate into different projects, and more flexible in its capabilities for human-agent interaction, based upon the experience gathered over the years across different virtual environments and human-robot interaction scenarios. As a result, this work makes several different contributions to the field of Agent-Based Architectures. More precisely, the FAtiMA Toolkit’s library-based design allows developers to easily integrate it with other frameworks, its meta-cognitive model affords different internal reasoners and affective components, and its explicit dialogue structure gives control to the author even within highly complex scenarios. To demonstrate the use of the FAtiMA Toolkit, several different use cases where the toolkit was successfully applied are described and discussed.},
journal = {ACM Trans. Interact. Intell. Syst.},
month = mar,
articleno = {8},
numpages = {30},
keywords = {Embodied agents, affective computing, cognitive architecture, emotions, social robots}
}

@inproceedings{10.1145/3475957.3484446,
author = {Baird, Alice and Stappen, Lukas and Christ, Lukas and Schumann, Lea and Messner, Eva-Maria and Schuller, Bj\"{o}rn W.},
title = {A Physiologically-Adapted Gold Standard for Arousal during Stress},
year = {2021},
isbn = {9781450386784},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3475957.3484446},
doi = {10.1145/3475957.3484446},
abstract = {Emotion is an inherently subjective psycho-physiological human state and to produce an agreed-upon representation (gold standard) for continuously perceived emotion requires time-consuming and costly training of multiple human annotators. With this in mind, there is strong evidence in the literature that physiological signals are an objective marker for states of emotion, particularly arousal. In this contribution, we utilise a multimodal dataset captured during a Trier Social Stress Test to explore the benefit of fusing physiological signals - Heartbeats per Minute ($BPM$), Electrodermal Activity (EDA), and Respiration-rate - for recognition of continuously perceived arousal utilising a Long Short-Term Memory, Recurrent Neural Network architecture, and various audio, video, and textual based features. We use the MuSe-Toolbox to create a gold standard that considers annotator delay and agreement weighting. An improvement in Concordance Correlation Coefficient (CCC) is seen across features sets when fusing EDA with arousal, compared to the arousal only gold standard results. Additionally, BERT-based textual features' results improved for arousal plus all physiological signals, obtaining up to .3344 CCC (.2118 CCC for arousal only). Multimodal fusion also improves CCC. Audio plus video features obtain up to .6157 CCC for arousal plus EDA, BPM.},
booktitle = {Proceedings of the 2nd on Multimodal Sentiment Analysis Challenge},
pages = {69–73},
numpages = {5},
keywords = {affective computing, multimodal fusion, stress},
location = {Virtual Event, China},
series = {MuSe '21}
}

@inproceedings{10.1145/3308532.3329419,
author = {Delamarre, Alban and Buche, C\'{e}dric and Lisetti, Christine},
title = {AIMER: Appraisal Interpersonal Model of Emotion Regulation, Affective Virtual Students to Support Teachers Training},
year = {2019},
isbn = {9781450366724},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308532.3329419},
doi = {10.1145/3308532.3329419},
abstract = {Elementary school classrooms are emotionally stressful environments, for both students and teachers. Successful teachers use strategies that regulate students' emotions and behaviors while also controlling their own emotions (stress, nervousness). To prepare teachers for the challenges of teaching, teacher training should include emotional and behavioral management strategies. Virtual Training Environments (VTEs) are effective at providing experiences and increasing learning in many domains. Creating VTEs for teachers can improve student learning and teacher retention. We introduce our current research aimed at integrating emotionally-intelligent virtual students within a 3D classroom training system. In our simulation, virtual students' emotional states will be determined from an appraisal process of actions taken by the teacher trainee in the virtual classroom. Virtual students will then display the appropriate non-verbal behaviors and react to the teacher accordingly. We present the first steps required to implement our proposed architecture which are based on appraisal theory of emotions and emotion regulation theory.},
booktitle = {Proceedings of the 19th ACM International Conference on Intelligent Virtual Agents},
pages = {182–184},
numpages = {3},
keywords = {affective computing, appraisal theory, classroom simulators, emotion regulation, virtual agents},
location = {Paris, France},
series = {IVA '19}
}

@inproceedings{10.1145/3394171.3413570,
author = {Mittal, Trisha and Bhattacharya, Uttaran and Chandra, Rohan and Bera, Aniket and Manocha, Dinesh},
title = {Emotions Don't Lie: An Audio-Visual Deepfake Detection Method using Affective Cues},
year = {2020},
isbn = {9781450379885},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3394171.3413570},
doi = {10.1145/3394171.3413570},
abstract = {We present a learning-based method for detecting real and fake deepfake multimedia content. To maximize information for learning, we extract and analyze the similarity between the two audio and visual modalities from within the same video. Additionally, we extract and compare affective cues corresponding to perceived emotion from the two modalities within a video to infer whether the input video is "real" or "fake". We propose a deep learning network, inspired by the Siamese network architecture and the triplet loss. To validate our model, we report the AUC metric on two large-scale deepfake detection datasets, DeepFake-TIMIT Dataset and DFDC. We compare our approach with several SOTA deepfake detection methods and report per-video AUC of 84.4\% on the DFDC and 96.6\% on the DF-TIMIT datasets, respectively. To the best of our knowledge, ours is the first approach that simultaneously exploits audio and video modalities and also perceived emotions from the two modalities for deepfake detection.},
booktitle = {Proceedings of the 28th ACM International Conference on Multimedia},
pages = {2823–2832},
numpages = {10},
keywords = {affective computing, audio-visual, deepfakes, emotions, multimedia forensics},
location = {Seattle, WA, USA},
series = {MM '20}
}

@inproceedings{10.5555/2615731.2615836,
author = {Saunier, Julien and Jones, Haza\"{e}l},
title = {Mixed agent/social dynamics for emotion computation},
year = {2014},
isbn = {9781450327381},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {Affective computing is the study and development of systems and devices that can recognise, interpret, process, and simulate human affects. In this context, computational modelling of emotion is a major challenge in order to design believable virtual humans. This factor has an impact on both the individual behaviour and the collective one. Recently, researchers have shown an increased interest in the emotion contagion phenomenon in order to model emerging group behaviour.Stemming from works on multi-agent systems environments, we propose an architecture to manage both internal and external emotion dynamics. Emotions evolve in function of three influences: punctual events, temporal dynamics and external influences. In an embodied agent approach, the first is the responsibility of the agent's mind, the second of the agent's body, and the third of the environment. This functional architecture is then adapted to a multi-agent architecture, adding a control responsibility to the agent body. Finally, we show the results of several experiments to examine the properties of the architecture and its efficiency by comparing it to a full agent approach.},
booktitle = {Proceedings of the 2014 International Conference on Autonomous Agents and Multi-Agent Systems},
pages = {645–652},
numpages = {8},
keywords = {architecture, embodied agent, emotional contagion, multi-agent systems},
location = {Paris, France},
series = {AAMAS '14}
}

@inproceedings{10.1145/3706598.3713571,
author = {Chen, Xingtong and Wang, Xia and Fang, Cong and Fang, Le and Gong, Wei and Liu, Chengzhong and Wang, Stephen Jia},
title = {Emotion-aware Design in Automobiles: Embracing Technology Advancements to Enhance Human-vehicle Interaction},
year = {2025},
isbn = {9798400713941},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3706598.3713571},
doi = {10.1145/3706598.3713571},
abstract = {The integration of emotion-aware systems in vehicles is accelerated by new technologies, including advancements in AI and ubiquitous sensing technologies. As the automotive industry shifts from technology-centred, feature-driven approaches to human-centred design, this research focuses on how to effectively incorporate emotion features into user-centred design to enhance effective human-vehicle interaction in practices. By conducting an interview study with 31 industrial design practitioners, supplemented by insights from engineers and AI experts involved in the early-stage design and development of novel in-vehicle user interfaces and systems, we examined current practices, and sampled their challenges, attitudes and expectations related to emotion-aware systems. Our findings provide critical insights to the design space of emotion-aware systems from both user and AI perspectives, inform efforts to support design practices in this evolving area, and identify opportunities for future innovation in emotion-aware in-vehicle design. Based on our findings, we propose adaptations to design practices and recommendations for further research.},
booktitle = {Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems},
articleno = {783},
numpages = {18},
keywords = {Human-centred Design, Emotion, Human-vehicle Interaction},
location = {
},
series = {CHI '25}
}

@inproceedings{10.1145/3436369.3437425,
author = {Xu, Ke and Liu, Bin and Tao, Jianhua and Lv, Zhao and Li, Qifei and Fan, Cunhang},
title = {AMINN: Attention-Based Multi-Information Neural Network for Emotion Recognition},
year = {2021},
isbn = {9781450387835},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3436369.3437425},
doi = {10.1145/3436369.3437425},
abstract = {Most of text emotion recognition methods are mainly based on the combination of simple word embedding and single network model, which cannot effectively capture the semantic emotional relationship in the sentence. To address this issue, we propose a neural network architecture called AMINN (Attention-Based Multi-Information Neural Network). We apply the attention mechanism to dynamically integrate the semantic emotional information to obtain key emotional information. The proposed method consists of pre-trained word embedding with fine-tuned training, two subnetworks and attention mechanism to capture the semantic emotional information in the text. The CNN sub-network captures the local semantic emotional information of the text, the BiLSTM subnetwork captures the contextual semantic emotional information of the text. The attention mechanism is used to fuse these two subnetworks. We conduct experiments on both Chinese(nlpcc2014) and English(SST) datasets to evaluate the effectiveness of our methods. Experimental results show that the proposed model is better than all baseline methods and is not limited by language. It pushes the recognition accuracy of single sentence positive/negative classification from 79\% to 86\%, and the f1 value reaches the highest value of 0.78. The recognition performance of fine-grained sentiment labels was also improved by 9.6\%. These experimental results prove the superiority of our model.},
booktitle = {Proceedings of the 2020 9th International Conference on Computing and Pattern Recognition},
pages = {56–62},
numpages = {7},
keywords = {Multimodel fusion, attention mechanism, contextual embeddings, emotion recognition, natural language processing},
location = {Xiamen, China},
series = {ICCPR '20}
}

@inproceedings{10.1145/3267799.3267801,
author = {Dudzik, Bernd and Hung, Hayley and Neerincx, Mark and Broekens, Joost},
title = {Artificial Empathic Memory: Enabling Media Technologies to Better Understand Subjective User Experience},
year = {2018},
isbn = {9781450359788},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3267799.3267801},
doi = {10.1145/3267799.3267801},
abstract = {An essential part of being an individual is our personal history, in particular our episodic memories. Episodic memories revolve around events that took place in a person's past and are typically defined by a time, place, emotional associations, and other contextual information. They form an important driver for our emotional and cognitive interpretation of what is currently happening. This includes interactions with media technologies. However, current approaches for personalizing interactions with these technologies are neither aware of what episodic memories are triggered in users, nor of their emotional interpretations of those memories. We argue that this is a serious limitation, because it prevents applications from correctly estimating users' experiences. In short, such technologies lack empathy. In this position paper, we argue that media technologies need an Artificial Empathic Memory (AEM) of their users to address this issue. We propose a psychologically inspired architecture, examine the challenges to be solved, and highlight how existing research can become a starting point for overcoming them.},
booktitle = {Proceedings of the 2018 Workshop on Understanding Subjective Attributes of Data, with the Focus on Evoked Emotions},
pages = {1–8},
numpages = {8},
keywords = {user modeling, personalization, media-evoked emotions, episodic memory, empathic technology, affective computing},
location = {Seoul, Republic of Korea},
series = {EE-USAD'18}
}

@inproceedings{10.1145/3125739.3125772,
author = {Elfaramawy, Nourhan and Barros, Pablo and Parisi, German I. and Wermter, Stefan},
title = {Emotion Recognition from Body Expressions with a Neural Network Architecture},
year = {2017},
isbn = {9781450351133},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3125739.3125772},
doi = {10.1145/3125739.3125772},
abstract = {The recognition of emotions plays an important role in our daily life and is essential for social communication. Although multiple studies have shown that body expressions can strongly convey emotional states, emotion recognition from body motion patterns has received less attention than the use of facial expressions. In this paper, we propose a self-organizing neural architecture that can effectively recognize affective states from full-body motion patterns. To evaluate our system, we designed and collected a data corpus named the Body Expressions of Emotion (BEE) dataset using a depth sensor in a human-robot interaction scenario. For our recordings, nineteen participants were asked to perform six different emotions:anger, fear, happiness, neutral, sadness, and surprise. In order to compare our system with human-like performance, we conducted an additional experiment by asking fifteen annotators to label depth map video sequences as one of the six emotion classes. The labeling results from human annotators were compared to the results predicted by our system. Experimental results showed that the recognition accuracy of the system was competitive with human performance when exposed to body motion patterns from the same dataset.},
booktitle = {Proceedings of the 5th International Conference on Human Agent Interaction},
pages = {143–149},
numpages = {7},
keywords = {neural networks, learning systems, emotion recognition},
location = {Bielefeld, Germany},
series = {HAI '17}
}

@inproceedings{10.1145/3503161.3547886,
author = {Li, Juncheng and Xie, Junlin and Zhu, Linchao and Qian, Long and Tang, Siliang and Zhang, Wenqiao and Shi, Haochen and Zhang, Shengyu and Wei, Longhui and Tian, Qi and Zhuang, Yueting},
title = {Dilated Context Integrated Network with Cross-Modal Consensus for Temporal Emotion Localization in Videos},
year = {2022},
isbn = {9781450392037},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3503161.3547886},
doi = {10.1145/3503161.3547886},
abstract = {Understanding human emotions is a crucial ability for intelligent robots to provide better human-robot interactions. The existing works are limited to trimmed video-level emotion classification, failing to locate the temporal window corresponding to the emotion. In this paper, we introduce a new task, named Temporal Emotion Localization in videos (TEL), which aims to detect human emotions and localize their corresponding temporal boundaries in untrimmed videos with aligned subtitles. TEL presents three unique challenges compared to temporal action localization: 1) The emotions have extremely varied temporal dynamics; 2) The emotion cues are embedded in both appearances and complex plots; 3) The fine-grained temporal annotations are complicated and labor-intensive. To address the first two challenges, we propose a novel dilated context integrated network with a coarse-fine two-stream architecture. The coarse stream captures varied temporal dynamics by modeling multi-granularity temporal contexts. The fine stream achieves complex plots understanding by reasoning the dependency between the multi-granularity temporal contexts from the coarse stream and adaptively integrates them into fine-grained video segment features. To address the third challenge, we introduce a cross-modal consensus learning paradigm, which leverages the inherent semantic consensus between the aligned video and subtitle to achieve weakly-supervised learning. We contribute a new testing set with 3,000 manually-annotated temporal boundaries so that future research on the TEL problem can be quantitatively evaluated. Extensive experiments show the effectiveness of our approach on temporal emotion localization. The repository of this work is at https://github.com/YYJMJC/TemporalEmotion-Localization-in-Videos.},
booktitle = {Proceedings of the 30th ACM International Conference on Multimedia},
pages = {5083–5092},
numpages = {10},
keywords = {weakly-supervised temporal emotion localization, video-and-language understanding, cross-modal consensus},
location = {Lisboa, Portugal},
series = {MM '22}
}

@inproceedings{10.1145/3627341.3630391,
author = {Luo, Yiheng and Yang, Hui and Fan, Hongbin and Li, Muzi},
title = {Analysis of Sentiment Trends Based on Deep Learning},
year = {2023},
isbn = {9798400708701},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3627341.3630391},
doi = {10.1145/3627341.3630391},
abstract = {As the main expression medium of emotion, facial expression is used to explore personal emotion through facial expression recognition. The convolutional neural network architecture in deep learning can combine the emotion feature extraction and classification process, showing great advantages in emotion recognition by reducing the amount of computation and reference. In this paper, a deep learning-based facial expression recognition algorithm is proposed to realize expression analysis. The structure of multi-task convolutional neural network (MTCNN) is analyzed, the feature pyramid structure is introduced to design the C-MTCNN optimization algorithm, and the Inception v3 model is used to complete the feature extraction and emotion classification. The optimization of PCA algorithm and parameter adjustment on P-Net greatly improves the recognition accuracy of captured images, so as to realize the analysis of personal emotions. By testing the algorithm, the accuracy of the algorithm and the effect of the system are verified.},
booktitle = {Proceedings of the 2023 International Conference on Computer, Vision and Intelligent Technology},
articleno = {15},
numpages = {9},
location = {Chenzhou, China},
series = {ICCVIT '23}
}

@inproceedings{10.1145/3194085.3194094,
author = {V\"{o}gel, Hans-J\"{o}rg and S\"{u}\ss{}, Christian and Hubregtsen, Thomas and Ghaderi, Viviane and Chadowitz, Ronee and Andr\'{e}, Elisabeth and Cummins, Nicholas and Schuller, Bj\"{o}rn and H\"{a}rri, J\'{e}r\^{o}me and Troncy, Rapha\"{e}l and Huet, Benoit and \"{O}nen, Melek and Ksentini, Adlen and Conradt, J\"{o}rg and Adi, Asaf and Zadorojniy, Alexander and Terken, Jacques and Beskow, Jonas and Morrison, Ann and Eng, Kynan and Eyben, Florian and Moubayed, Samer Al and M\"{u}ller, Susanne},
title = {Emotion-awareness for intelligent vehicle assistants: a research agenda},
year = {2018},
isbn = {9781450357395},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194085.3194094},
doi = {10.1145/3194085.3194094},
abstract = {EVA1 is describing a new class of emotion-aware autonomous systems delivering intelligent personal assistant functionalities. EVA requires a multi-disciplinary approach, combining a number of critical building blocks into a cybernetics systems/software architecture: emotion aware systems and algorithms, multimodal interaction design, cognitive modelling, decision making and recommender systems, emotion sensing as feedback for learning, and distributed (edge) computing delivering cognitive services.},
booktitle = {Proceedings of the 1st International Workshop on Software Engineering for AI in Autonomous Systems},
pages = {11–15},
numpages = {5},
keywords = {privacy preserving machine learning, neuromorphic emotion sensing, multi-modal interaction design, intelligent assistants, emotional state analysis, emotion awareness, cognitive models and proactive recommendations, IoT},
location = {Gothenburg, Sweden},
series = {SEFAIS '18}
}

@inproceedings{10.1145/3136755.3143006,
author = {Pini, Stefano and Ahmed, Olfa Ben and Cornia, Marcella and Baraldi, Lorenzo and Cucchiara, Rita and Huet, Benoit},
title = {Modeling multimodal cues in a deep learning-based framework for emotion recognition in the wild},
year = {2017},
isbn = {9781450355438},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3136755.3143006},
doi = {10.1145/3136755.3143006},
abstract = {In this paper, we propose a multimodal deep learning architecture for emotion recognition in video regarding our participation to the audio-video based sub-challenge of the Emotion Recognition in the Wild 2017 challenge. Our model combines cues from multiple video modalities, including static facial features, motion patterns related to the evolution of the human expression over time, and audio information. Specifically, it is composed of three sub-networks trained separately: the first and second ones extract static visual features and dynamic patterns through 2D and 3D Convolutional Neural Networks (CNN), while the third one consists in a pretrained audio network which is used to extract useful deep acoustic signals from video. In the audio branch, we also apply Long Short Term Memory (LSTM) networks in order to capture the temporal evolution of the audio features. To identify and exploit possible relationships among different modalities, we propose a fusion network that merges cues from the different modalities in one representation. The proposed architecture outperforms the challenge baselines (38.81 \% and 40.47 \%): we achieve an accuracy of 50.39 \% and 49.92 \% respectively on the validation and the testing data.},
booktitle = {Proceedings of the 19th ACM International Conference on Multimodal Interaction},
pages = {536–543},
numpages = {8},
keywords = {Multimodal Deep Learning, Emotion Recognition, EmotiW 2017 Challenge, Convolutional Neural Networks},
location = {Glasgow, UK},
series = {ICMI '17}
}

@inproceedings{10.5555/2484920.2485037,
author = {Campano, Sabrina and Sabouret, Nicolas and de Sevin, Etienne and Corruble, Vincent},
title = {An evaluation of the COR-E computational model for affective behaviors},
year = {2013},
isbn = {9781450319935},
publisher = {International Foundation for Autonomous Agents and Multiagent Systems},
address = {Richland, SC},
abstract = {The simulation of believable behaviors for virtual agents requires to take human factors such as emotions into account. Most computational models dealing with this issue include emotion categories in their architecture. However, determining categories to use and their influence on behavior is a difficult task. In order to address this challenge, our COR-E model uses an architecture without emotion categories. In this paper, we present an evaluation of this model in the context of a waiting line scenario. We show that COR-E can produce believable emotional behaviors, and test the contributions of the various components and characteristics of its architecture to these positive results.},
booktitle = {Proceedings of the 2013 International Conference on Autonomous Agents and Multi-Agent Systems},
pages = {745–752},
numpages = {8},
keywords = {virtual agent, emotion, believability, behavior, affect},
location = {St. Paul, MN, USA},
series = {AAMAS '13}
}

@inbook{10.1145/3122865.3122866,
title = {Preface},
year = {2017},
isbn = {9781970001075},
publisher = {Association for Computing Machinery and Morgan \&amp; Claypool},
url = {https://doi.org/10.1145/3122865.3122866},
abstract = {The field of multimedia is dedicated to research and studies that leverage multiple modalities of signals and data in developing intelligent systems and technologies. Be it search engine, recommendation system, streaming service, interactive agent, or collaborative system, multimedia plays a critical role in ensuring full understanding of multimodal sensory signals, robust modeling of user-content interaction, natural and rich communication experience, and scalable system deployment. The goal is to utilize unique contributions from each modality, integrate complementary synergies, and achieve the best performance and novel functions beyond what's separately available in each individual medium. In this community, most contributors also maintain strong activities in other disciplines such as networking, computer vision, human-computer interaction, and machine learning. But the field of multimedia is unique in offering a rich and dynamic forum for researchers from "traditional" fields to collaborate and develop new solutions and knowledge that transcend the boundaries of individual disciplines.The field enjoys a long history of vibrant research. For example, the flagship ACM SIGMM Multimedia Conference was established in 1993, celebrating its 25th anniversary this year. The community also has several well-known conferences and journals organized by ACM, IEEE, and other groups, attracting a large number of researchers and practitioners from around the world. However, despite the prolific research activities and outcomes, there has been less effort toward developing books that serve as an introduction to the rich spectrum of topics in this broad field. Most of the few books available today either focus on specific subfields or basic background. There is a lack of tutorial-style materials covering the active topics being pursued by the leading researchers at frontiers of the field.SIGMM launched a new initiative to address this need in 2015, by selecting and inviting 12 rising-star speakers from different subfields of multimedia to deliver plenary tutorial style talks at ACM Multimedia 2015. Each speaker discussed challenges and the state of the art within their prospective research areas in a general manner to the broad community. Topics covered were comprehensive, including multimedia content understanding, multimodal human-human and human-computer interaction, multimedia social media, and multimedia system architecture and deployment. Following the very positive responses to the talks, these rising-star speakers were invited to expand the content covered in their talks to chapters that can be used as reference materials for researchers, students, and practitioners. Each resulting chapter discusses problems, technical challenges, state-of-the-art approaches and performances, open issues, and promising directions for future work. Collectively, the chapters provide an excellent sampling of major topics addressed by the community as a whole. This book, capturing outcomes of such efforts, is well positioned to fill the aforementioned needs by providing tutorial-style reference materials for frontier topics of multimedia.Section 1 of the book includes five chapters that are focused on analysis and understanding of multimedia content. Topics covered range from analysis of video content, audio content, multimodal content about interaction of freestanding conversational groups, and analysis of multimedia data in the encrypted format for preserving privacy on cloud servers, to efficient approximate similarity search techniques for searching over large-scale databases.First, Zuxuan Wu et al. review current research on understanding video content by detecting the classes of actions or events contained in a given video clip and generation of full-sentence captions describing the content in each such video. Unlike previous surveys, this review focuses on solutions based on deep learning, reflecting the recent trend of research in this area. The chapter also gives extensive reviews of the datasets used in state-of-the-art research and benchmarking efforts.Extending the modality from video to audio, in Chapter 2, Gerald Friedland et al. introduce the field of computer audition, aiming to develop the theory behind artificial systems that can extract information from sound. This chapter reviews the research datasets available, appropriate representations needed for audio, and a few challenging problems such as automatic extraction of hierarchical semantic structures from audio content and automatic discovery of high-level semantic concepts from massive audio data and associated metadata.The holy grail of research for the multimedia community is to be able to integrate and fuse information extracted from multiple modalities of data. In Chapter 3, Xavier Alameda-Pineda et al. present an excellent example and emergent research challenges in the application of detecting social interaction among freestanding conversational groups. The chapter includes overviews of research issues, approaches, evaluation of joint estimation of head and body poses using multiPreface modality data (such as wearable sensors and distributed camera networks), and results of detecting dynamic group formation of interacting people.Chapter 4 addresses a novel emerging topic prompted by the popular approach to multimedia analysis using cloud computing servers. When multimedia data is sent to the cloud for storage or processing, there is a risk of privacy breach via unauthorized access by third parties to the content in the cloud. Pradeep Atrey et al. review state-of-the-art methods and open issues for processing multimedia content in the encrypted domain without needing to convert data to the original format. This allows content to stay in its protected form while useful analysis is performed on it.In Chapter 5, Herv\'{e}e Jeundefinedou surveys efficient techniques for finding approximate solutions for similarity search, which is of particular interest when searching massive multimedia data like images, videos, and audio recordings. Jeundefinedou considers various performance factors like query speed, memory requirement, and search accuracy. Multiple frameworks based on locality sensitive hashing (LSH), quantization/ compression, and hybrid combinations are also reviewed in a coherent manner.In Section 2 of the book, the emphasis shifts from content analysis to humancentered aspects of multimedia computing. This new focus goes beyond extraction of semantic information from multimedia data. Instead, the broad research scope incorporates understanding of users and user-content interaction so as to improve effectiveness of multimedia systems in many applications, such as search and recommendation.Under the human-centric theme, Chapter 6, authored by Peng Cui, discusses the evolution of multimedia computing paradigms from the data-centric, to the content-centric, and recently to the human-centric. Cui presents a new framework, called social-sensed multimedia computing, to capture many key issues involved and advances achieved, including understanding of user-content interaction behavior, understanding of user intent, multimedia representation considering user intention, and integration of heterogeneous data sensed on multimedia social networks.Chapter 7 follows the human-centric theme and further moves the focus from processing individual multimedia data streams to processing a large number of heterogeneous streams in different modalities involving a large number of people. Analysis of such massive streams offers the possibility of detecting important situations of society, such as socio-economic affairs, as well as the living environment. Vivek Singh provides an overview of the problem definition, research framework, and the EventShop toolkit he developed for application development in this emerging area.The extension to the human-centric computing paradigm also calls for formal mathematical theories and tools for explaining the phenomena observed, such as the information propagation behaviors and the occurrences of information cascades on social networks. In Chapter 8, Marian-Andrei Rizoiu et al. review stochastic processes such as the Hawkes point process for modeling discrete, interdependent events over continuous time. These are strongly related to patterns corresponding to retweet cascade events on social media. Successful models like these can help researchers understand information dissemination patterns and predict popularity on social media.Interaction between users and content reveals not only the intent of the user (covered in Chapter 6), but also attributes of the content as well as of the user him/herself. Such interaction can be manifested in multiple forms including explicit cues such as visual and verbal expressions, and implicit cues such as eye movement and physiological signals like brain activity and heart rate. Chapter 9 includes a survey by Subramanian Ramanathan et al. on how such implicit user interaction cues can be explored to improve analysis of content (scene understanding) and user (user emotion recognition).To support research and development of emerging multimedia topics discussed above, there is a critical need for new generations of communication and computing systems that take into account the unique requirements of multimedia, such as real-time, high bandwidth, distributiveness, major power consumption, and resource uncertainty. The popular cloud-based computing systems, though prevalent formanyapplications, are not suitable for large-scale multimedia applications such as cloud-based gaming service and animation rendering service.The last section of the book focuses on the systems aspect, covering distinct topics of multimedia fog computing (Chapter 10) and cloud gaming (Chapter 11). Cheng-Hsin Hsu et al. survey the emerging paradigm focused on fog computing, in which computing services are crowdsourced to the edge nodes or even to the client devices on the user end. This offers major potential benefits in terms of low latency, location awareness, scalability, and heterogeneity. However, it also poses many significant challenges in areas such as resource discovery, resource allocation and management, quality of service, and security. Discussion of these challenges, along with recent advances in this area, are presented in Chapter 10.Finally, as a concrete example of large-scale distributed multimedia computing systems, Chapter 11 (by Kuan-Ta Chen et al.) presents a comprehensive survey of cloud gaming, with emphasis on the development of platform and testbed, test scenarios, and evaluation of performance, in order to enhance optimal design of various components of the complex cloud gaming systems. In particular, the chapter overviews extensive research in areas such as open-source platforms, cloud deployment, client design, and communication between gaming servers and clients.The scope of this book is by no means exhaustive or complete. For example, it can be expanded to include other important topics such as (but not limited to) multimedia content generation, multimodal knowledge discovery and representation, multimedia immersive networked environments, and applications in areas like healthcare, learning, and infrastructure. Nonetheless, the comprehensive survey materials already covered in the book provide an excellent foundation for exploring additional topics mentioned above, and many other relevant fields.We would like to give sincere acknowledgment to Dr. Svebor Karaman, who has provided tremendous assistance in communicating with contributors and organizing the content of this book. In addition, Diane Cerra and her team at Morgan \&amp; Claypool Publishers have provided valuable guidance and editorial help},
booktitle = {Frontiers of Multimedia Research},
pages = {xi–xv}
}

@inproceedings{10.1145/3448696.3448700,
author = {Itenge, Helvi and Sedano, Carolina Islas and Winschiers-Theophilus, Heike},
title = {Virtually Escaping Lock Down - co-designing a mixed reality escape room narrative with Namibian learners-},
year = {2021},
isbn = {9781450388696},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3448696.3448700},
doi = {10.1145/3448696.3448700},
abstract = {The Covid-19 pandemic and consequent lock down measures, disrupted our ongoing co-design of a mixed-reality escape room with grade 6 and 7 learners from a public school in Namibia. Consequentially, we launched into a remote participatory escape room narrative process with 10 learners reachable via mobile phones over a period of five months. Following a guided escape room narrative procedure we adapted activities progressively within an iterative approach of task completion, analysis, discussion and reflection. Student responses were analysed using Ekman’s emotion theory, to determine dominate emotions. We discuss how the prevalent emotions identified are to be guiding the final design of the narrative as well as the puzzles for an engaging mixed reality escape room experience. We further share our learning in accounting for technical as well as conceptual challenges in the remote interactions with the children considering the lack of resources as well as how the lock-down has affected the narrative design.},
booktitle = {Proceedings of the 3rd African Human-Computer Interaction Conference: Inclusiveness and Empowerment},
pages = {103–112},
numpages = {10},
keywords = {Namibia, children, co-design, escape room, remote interactions},
location = {Maputo, Mozambique},
series = {AfriCHI '21}
}

@inproceedings{10.1145/2221924.2221943,
author = {Serbedzija, Nikola and Bertolotti, Gian Mario},
title = {Adaptive and personalized body networking},
year = {2010},
isbn = {9781450300292},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/2221924.2221943},
doi = {10.1145/2221924.2221943},
abstract = {Body networking calls for novel methods and tools for a tight and implicit man machine confluence. One way to achieve this is to make technical systems sensitive and reactive to user emotional, cognitive or physical situation. This paper describes service-oriented software architecture, designed to respond to the user via a biocybernetic loop that transforms changes in user behaviour into services that may be incorporated into a highly personalized user-centric system. These services are used to drive real-time system adaptation tailored to a specific individual in a particular usage context.},
booktitle = {Proceedings of the Fifth International Conference on Body Area Networks},
pages = {91–97},
numpages = {7},
keywords = {service oriented architectures, physiological computing, ontology, closed loop control, body area networks},
location = {Corfu, Greece},
series = {BodyNets '10}
}

@inproceedings{10.1145/3462244.3479940,
author = {Maman, Lucien and Likforman-Sulem, Laurence and Chetouani, Mohamed and Varni, Giovanna},
title = {Exploiting the Interplay between Social and Task Dimensions of Cohesion to Predict its Dynamics Leveraging Social Sciences},
year = {2021},
isbn = {9781450384810},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3462244.3479940},
doi = {10.1145/3462244.3479940},
abstract = {Emergent states are behavioral, cognitive and affective processes appearing among the members of a group when they interact together. In the last decade, the development of computational approaches received a growing interest in building Human-Centered systems. Such a development is particularly difficult because some of these states have several dimensions interplaying somehow and somewhere over time. In this paper, we focus on cohesion, its dimensions and their interplay. Several definitions of cohesion exist, it can be simply defined as the tendency of a group to stick together to pursue goals and/or affective needs. This plethora of definitions resulted in many different cohesion dimensions. Social and Task dimensions are the most investigated both in Social Sciences and Computer Science since they both play an important role in a wide range of contexts and groups. To the best of our knowledge, however, no previous work on the prediction of cohesion dynamics focused on how these 2 dimensions interplay. We leverage Social Sciences to address this issue. In particular, we take advantage of the importance of Social cohesion for creating flexible and constructive relationships to reinforce Task cohesion. We describe a Deep Neural Network architecture (DNN) for predicting the dynamics of Task cohesion by applying transfer learning from a pre-trained model dedicated to the prediction of Social cohesion dynamics. Our architecture is evaluated against several baselines. Results show that it significantly improves the predictions of the Task cohesion dynamics, confirming the benefits of integrating Social Sciences insights into models architectures.},
booktitle = {Proceedings of the 2021 International Conference on Multimodal Interaction},
pages = {16–24},
numpages = {9},
keywords = {Transfer Learning, Social Signal Processing, Multimodal Interaction, Group Dynamics, Cohesion},
location = {Montr\'{e}al, QC, Canada},
series = {ICMI '21}
}

@inproceedings{10.1145/3160504.3160543,
author = {Hayashi, Elaine C. S. and Baranauskas, M. Cec\'{\i}lia C.},
title = {Accessibility and affect in technologies for museums: a path towards socio-enactive systems},
year = {2017},
isbn = {9781450363778},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3160504.3160543},
doi = {10.1145/3160504.3160543},
abstract = {The ubiquitous presence of digital technology in daily activities and our bodily immersion in such activities present new and interesting opportunities for research in Human-computer interaction. Immersive and interactive solutions for art exhibitions and museums can enhance visitors' experiences, providing for richer affective and cognitive experiences. However, most of the existing solutions are not suitable for all people, especially visitors with physical impairment. Many museums in Brazil do not even offer possibilities for visitors with visual disabilities to enter and locate themselves. This paper informs norms and solutions for accessibility in museums and starts a discussion on new opportunities for investigation in HCI, especially in accessible information visualization and socio-enactive systems.},
booktitle = {Proceedings of the XVI Brazilian Symposium on Human Factors in Computing Systems},
articleno = {7},
numpages = {10},
keywords = {universal design, tactile map, tactile floor, socio-enactive systems, information visualization, enactive systems, accessibility, HCI, Affectibility},
location = {Joinville, Brazil},
series = {IHC '17}
}

@article{10.5555/945365.945377,
author = {Gadanho, Sandra Clara},
title = {Learning behavior-selection by emotions and cognition in a multi-goal robot task},
year = {2003},
issue_date = {12/1/2003},
publisher = {JMLR.org},
volume = {4},
number = {null},
issn = {1532-4435},
abstract = {The existence of emotion and cognition as two interacting systems, both with important roles in decision-making, has been recently advocated by neurophysiological research (LeDoux, 1998, Damasio, 1994. Following that idea, this paper presents the ALEC agent architecture which has both emotive and cognitive learning, as well as emotive and cognitive decision-making capabilities to adapt to real-world environments. These two learning mechanisms embody very different properties which can be related to those of natural emotion and cognition systems. The reported experiments test ALEC within a simulated autonomous robot which learns to perform a multi-goal and multi-step survival task when faced with real world conditions, namely continuous time and space, noisy sensors and unreliable actuators. Experimental results show that both systems contribute positively to the learning performance of the agent.},
journal = {J. Mach. Learn. Res.},
month = dec,
pages = {385–412},
numpages = {28}
}



